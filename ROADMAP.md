# Development Roadmap

### MCP Server ğŸ–¥ï¸
- âœ… Python stdio server support
- âœ… node.js stdio server support
- âœ… http mcp server support
- ğŸ”² connecting mcp servers with json file as a standard way

### MCP Client ğŸ¤–
- âœ… Stdio client implementation
- âœ… message processing ability for LLM generation
- âœ… chatting ability
- âœ… support for connecting multiple servers

### Synthetic Data Generation ğŸ“
- âœ… Tool use task generation
    - âœ… single turn
    - ğŸ”² multi-turn
- âœ… Task verification

### Evaluation ğŸ“Š
- âœ… Implement core evaluation metrics (accuracy, latency)
- âœ… Create automated testing framework
- ğŸ”² Automatic Deep Evaluating 
- ğŸ”² Evaluating the implementation of MCP server

### Data Pipeline ğŸ”„
- âœ… Design unified data schema for all benchmarks
- âœ… Implement data preprocessing tools
- âœ… Add support for multiple data formats

### LLM Provider ğŸ§ 
- âœ… OpenAI API integration (used for data generation and testing)
- âœ… local vllm-based model 

### CLI ğŸ”§
- âœ… Task generator
- âœ… Task verifier
- âœ… Data converter
- âœ… Model evaluator
- âœ… Report generator
- âœ… Auto end-to-end evaluation

### Front-end ğŸ¨
- âœ… React application setup with TypeScript
- âœ… Core navigation and routing
- âœ… MCP server configuration interface
- âœ… Chat client for MCP interactions
- âœ… Task generation and verification UI
- âœ… Model evaluation dashboard
- âœ… Results and analytics pages
- âœ… Data management interfaces
- ğŸ”² Unifying the model config for all the pages and sharing the same component
- ğŸ”² Saving any existing model config as a config file and support load it again

## Issues
- Evluating multiple models does not working
- Analyze feature does not support not generating AI report
- Judge Rubrics select not generate report